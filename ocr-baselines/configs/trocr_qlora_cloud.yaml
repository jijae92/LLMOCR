# TrOCR QLoRA Training Configuration (Cloud GPU - CUDA Only)
# Requires: CUDA GPU with bitsandbytes support (NOT for macOS)
# Recommended: A10, A100, H100, or similar

# ⚠️  WARNING: QLoRA (4-bit) is NOT supported on macOS/Metal
# This configuration is for cloud GPU training only.
# After training, download adapters for local inference.

model:
  base_model: "microsoft/trocr-base-printed"
  load_in_4bit: true       # Enable 4-bit quantization
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"

lora:
  r: 64                    # Can use higher rank with QLoRA
  alpha: 128
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "out_proj"
    - "fc1"
    - "fc2"

data:
  train_json: "data/train.jsonl"
  val_json: "data/val.jsonl"
  max_target_length: 512

training:
  output_dir: "runs/trocr-qlora"
  per_device_train_batch_size: 8  # Can use larger batch with 4-bit
  per_device_eval_batch_size: 8
  learning_rate: 2.0e-4
  num_train_epochs: 3
  warmup_steps: 100
  gradient_accumulation_steps: 1

  # Evaluation
  eval_steps: 100
  save_steps: 200
  logging_steps: 10

  # Device
  device: "cuda"           # MUST be CUDA
  fp16: true               # Enable FP16 on CUDA

# Usage on cloud:
# 1. Train on cloud GPU with this config
# 2. Download only the adapter weights (small, ~tens of MB)
# 3. Load adapter on Mac for inference:
#    python -m src.cli --model trocr --adapter-path runs/trocr-qlora --image img.jpg
#
# Memory usage (4-bit): ~4-8GB VRAM for batch_size=8, r=64
