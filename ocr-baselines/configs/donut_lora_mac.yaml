# Donut LoRA Training Configuration (macOS/MPS Optimized)
# Designed for Apple Silicon Macs with 16GB+ Unified Memory

model:
  base_model: "naver-clova-ix/donut-base"
  # Note: Donut-base is larger than TrOCR-base

lora:
  r: 16                    # LoRA rank
  alpha: 32                # LoRA scaling factor
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "out_proj"
    - "fc1"
    - "fc2"
    - "qkv"               # Swin Transformer specific

data:
  train_json: "data/train.jsonl"
  val_json: "data/val.jsonl"
  max_target_length: 512

training:
  output_dir: "runs/donut-lora"
  per_device_train_batch_size: 1  # Donut is larger, use smaller batch
  per_device_eval_batch_size: 1
  learning_rate: 2.0e-4
  num_train_epochs: 3
  warmup_steps: 100
  gradient_accumulation_steps: 2   # Compensate for smaller batch

  # Evaluation
  eval_steps: 100
  save_steps: 200
  logging_steps: 10

  # Device
  device: "auto"
  no_mps: false
  fp16: false

# Memory optimization tips:
# - Donut requires more memory than TrOCR
# - Expected memory usage: ~12-16GB for batch_size=1, r=16
# - If OOM: Reduce lora.r to 8 or use gradient_checkpointing
